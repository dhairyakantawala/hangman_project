{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 313/313 [00:09<00:00, 34.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 2.6729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 313/313 [00:07<00:00, 42.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 2.3448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 313/313 [00:06<00:00, 46.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 2.2073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 313/313 [00:06<00:00, 46.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 2.1013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 313/313 [00:06<00:00, 47.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 2.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 313/313 [00:06<00:00, 46.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 loss: 1.9351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 313/313 [00:06<00:00, 46.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 loss: 1.8601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 313/313 [00:06<00:00, 47.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 loss: 1.7834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load a smaller subset if memory is an issue\n",
    "with open(\"words_250000_train.txt\", \"r\") as f:\n",
    "    word_list = [w.strip() for w in f if len(w.strip()) > 2 and len(w.strip()) <= 12]\n",
    "    word_list = random.sample(word_list, 20000)  # reduce to 20k words if needed\n",
    "\n",
    "# Build vocab\n",
    "all_chars = sorted(set(\"\".join(word_list)))\n",
    "char_to_idx = {ch: i + 1 for i, ch in enumerate(all_chars)}\n",
    "char_to_idx[\"_\"] = len(char_to_idx) + 1  # MASK\n",
    "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
    "PAD_IDX = 0\n",
    "vocab_size = len(char_to_idx) + 1\n",
    "\n",
    "# Dataset\n",
    "class HangmanDataset(Dataset):\n",
    "    def __init__(self, words, max_len=16):\n",
    "        self.data = []\n",
    "        self.max_len = max_len\n",
    "        for word in words:\n",
    "            idx = random.randint(0, len(word) - 1)\n",
    "            target = word[idx]\n",
    "            masked = list(word)\n",
    "            masked[idx] = \"_\"\n",
    "            self.data.append((\"\".join(masked), target, idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        masked, target, pos = self.data[idx]\n",
    "        x = [char_to_idx.get(c, 0) for c in masked]\n",
    "        x += [PAD_IDX] * (self.max_len - len(x))\n",
    "        return torch.tensor(x), torch.tensor(char_to_idx[target]), torch.tensor(pos)\n",
    "\n",
    "# Model\n",
    "class BiLSTMHangman(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, hidden_dim=64, layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(emb)\n",
    "        return self.fc(lstm_out)\n",
    "\n",
    "# Training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTMHangman(vocab_size).to(device)\n",
    "dataset = HangmanDataset(word_list)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(8):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y, pos in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        x, y, pos = x.to(device), y.to(device), pos.to(device)\n",
    "        out = model(x)  # [batch, seq_len, vocab_size]\n",
    "        logits = out[torch.arange(x.size(0)), pos]\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# Save model + mappings\n",
    "torch.save(model.state_dict(), \"bilstm_hangman.pth\")\n",
    "torch.save({\"char_to_idx\": char_to_idx, \"idx_to_char\": idx_to_char}, \"bilstm_vocab.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
