{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preprocessing\n",
    "\n",
    "Load words_250000_train.txt.\n",
    "Generate millions of (masked_word, correct_next_letter) pairs.\n",
    "\n",
    "### Step 2: Model\n",
    "\n",
    "Input: 1D array of character encodings (size ≈ word_length × channels).\n",
    "CNN Layers:\n",
    "Embedding (characters → vectors)\n",
    "1D Conv\n",
    "Global Max Pool\n",
    "Dense → softmax over 26 letters.\n",
    "\n",
    "### Step 3: Training\n",
    "\n",
    "Train on masked examples.\n",
    "Loss: CrossEntropy.\n",
    "\n",
    "### Step 4: Deployment\n",
    "\n",
    "At every guess:\n",
    "Pass masked word.\n",
    "Predict letter probabilities.\n",
    "Pick highest probability letter NOT YET guessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 1514082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|          | 31/2958 [00:02<03:38, 13.40it/s, loss=2.8796]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     89\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/new_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/new_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/new_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/new_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[4], line 77\u001b[0m, in \u001b[0;36mHangmanDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     76\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(y\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "MAX_LEN = 20\n",
    "CHAR2IDX = {c: i+1 for i, c in enumerate(string.ascii_lowercase)}\n",
    "CHAR2IDX['_'] = 27\n",
    "IDX2CHAR = {v: k for k, v in CHAR2IDX.items()}\n",
    "VOCAB_SIZE = 28  # 26 letters + _ + padding (0)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model\n",
    "class HangmanCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HangmanCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, 32, padding_idx=0)\n",
    "        self.conv1 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(128, 26)  # 26 letters output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(1,2)  # (batch, embed_dim, seq_len)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(2)  # (batch, 128)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Utility functions\n",
    "def encode_word(word):\n",
    "    return [CHAR2IDX.get(c, 0) for c in word]\n",
    "\n",
    "def pad_word(encoded):\n",
    "    if len(encoded) > MAX_LEN:\n",
    "        return encoded[:MAX_LEN]\n",
    "    return encoded + [0]*(MAX_LEN - len(encoded))\n",
    "\n",
    "def mask_word(word, mask_ratio=0.4):\n",
    "    masked = list(word)\n",
    "    num_to_mask = max(1, int(len(word) * mask_ratio))\n",
    "    indices = random.sample(range(len(word)), num_to_mask)\n",
    "    for idx in indices:\n",
    "        masked[idx] = '_'\n",
    "    return \"\".join(masked), [word[i] for i in indices]  # returns masked word and original letters masked\n",
    "\n",
    "# Load dictionary\n",
    "with open(\"/Users/dhairya/cs projects/trexquant assignment/words_250000_train.txt\") as f:\n",
    "    dictionary = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "# Prepare Dataset\n",
    "train_data = []\n",
    "for word in dictionary:\n",
    "    if not word.isalpha() or len(word) > MAX_LEN:\n",
    "        continue\n",
    "    for _ in range(2):  # augment: 2 masked versions per word\n",
    "        masked_word, hidden_letters = mask_word(word)\n",
    "        for letter in hidden_letters:\n",
    "            x = pad_word(encode_word(masked_word))\n",
    "            y = CHAR2IDX[letter]\n",
    "            train_data.append((x, y))\n",
    "\n",
    "print(f\"Total training samples: {len(train_data)}\")\n",
    "\n",
    "# Dataloader\n",
    "class HangmanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y-1, dtype=torch.long)  # 0-based\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(HangmanDataset(train_data), batch_size=512, shuffle=True)\n",
    "\n",
    "# Training\n",
    "model = HangmanCNN().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "    for x_batch, y_batch in progress_bar:\n",
    "        x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    print(f\"Epoch {epoch+1}: Loss {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"hangman_cnn.pth\")\n",
    "print(\"Model saved as hangman_cnn.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import re\n",
    "import collections\n",
    "\n",
    "# Constants for model\n",
    "MAX_LEN = 20\n",
    "CHAR2IDX = {c: i+1 for i, c in enumerate(string.ascii_lowercase)}\n",
    "CHAR2IDX['_'] = 27\n",
    "IDX2CHAR = {v: k for k, v in CHAR2IDX.items()}\n",
    "VOCAB_SIZE = 28\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Model class\n",
    "class HangmanCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HangmanCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, 32, padding_idx=0)\n",
    "        self.conv1 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(128, 26)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(1, 2)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(2)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Lazy load model\n",
    "_model = None\n",
    "def load_model():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        _model = HangmanCNN().to(DEVICE)\n",
    "        _model.load_state_dict(torch.load(\"hangman_cnn.pth\", map_location=DEVICE))\n",
    "        _model.eval()\n",
    "    return _model\n",
    "\n",
    "# Helper to encode input\n",
    "def encode_input(word):\n",
    "    cleaned = word[::2].replace(' ', '').lower()\n",
    "    encoded = [CHAR2IDX.get(c, 0) for c in cleaned]\n",
    "    if len(encoded) > MAX_LEN:\n",
    "        encoded = encoded[:MAX_LEN]\n",
    "    else:\n",
    "        encoded += [0] * (MAX_LEN - len(encoded))\n",
    "    return torch.tensor(encoded, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "# 🚀 THE REPLACED FUNCTION\n",
    "def guess(self, word):  # word example: \"_ p p _ e \"\n",
    "    model = load_model()\n",
    "\n",
    "    input_tensor = encode_input(word)  # (1, MAX_LEN)\n",
    "    logits = model(input_tensor)       # (1, 26)\n",
    "    probs = torch.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "\n",
    "    # Sort predictions by probability descending\n",
    "    letter_indices = probs.argsort()[::-1]\n",
    "\n",
    "    # Find the highest probability letter that hasn't been guessed yet\n",
    "    for idx in letter_indices:\n",
    "        letter = string.ascii_lowercase[idx]\n",
    "        if letter not in self.guessed_letters:\n",
    "            return letter\n",
    "\n",
    "    # Fallback (should rarely happen)\n",
    "    for letter in string.ascii_lowercase:\n",
    "        if letter not in self.guessed_letters:\n",
    "            return letter\n",
    "\n",
    "    return 'e'  # Emergency fallback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
