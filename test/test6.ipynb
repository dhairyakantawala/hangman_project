{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple, defaultdict\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# Experience Replay Buffer\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class HangmanRLAgent:\n",
    "    def __init__(self, vocab, max_lives=6, epsilon=1.0, gamma=0.99, lr=0.00025,\n",
    "                 epsilon_decay_steps=200000, # Slower decay based on total steps\n",
    "                 min_epsilon=0.05,\n",
    "                 buffer_size=50000,\n",
    "                 batch_size=128,\n",
    "                 target_update_freq=500): # Update target net less frequently (steps)\n",
    "        self.vocab = vocab\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "        self.idx_to_char = {i: ch for ch, i in self.char_to_idx.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "        self.max_word_len = 25\n",
    "        # State includes: one-hot encoded masked word + binary vector for guessed letters\n",
    "        self.state_dim = (self.vocab_size + 1) * self.max_word_len + self.vocab_size\n",
    "        self.hidden_size = 512 # Increased hidden size\n",
    "        self.output_size = self.vocab_size\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.policy_net = self.build_model().to(self.device)\n",
    "        self.target_net = self.build_model().to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval() # Target network is only for inference\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "        self.criterion = nn.SmoothL1Loss() # Huber Loss\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "        self.max_lives = max_lives\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_epsilon = epsilon\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps # Steps over which epsilon decays\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq # In terms of optimization steps\n",
    "\n",
    "        self.total_steps = 0\n",
    "        self.total_optim_steps = 0\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        # Deeper network\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size // 2), # Extra layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size // 2, self.output_size)\n",
    "        )\n",
    "\n",
    "    def encode_state(self, masked_word, guessed_letters):\n",
    "        # Encode masked word (one-hot + mask token)\n",
    "        word_state = []\n",
    "        for i in range(self.max_word_len):\n",
    "            one_hot = [0] * (self.vocab_size + 1)\n",
    "            if i < len(masked_word):\n",
    "                ch = masked_word[i]\n",
    "                if ch == '_':\n",
    "                    one_hot[-1] = 1 # Mask token\n",
    "                elif ch in self.char_to_idx:\n",
    "                    one_hot[self.char_to_idx[ch]] = 1\n",
    "                else:\n",
    "                    # Should not happen if dictionary is pre-filtered\n",
    "                    # print(f\"Warning: Unexpected character '{ch}' in masked word. Treating as mask.\")\n",
    "                    one_hot[-1] = 1 # Treat unknown revealed char as mask\n",
    "            else:\n",
    "                 one_hot[-1] = 1 # Pad with mask token\n",
    "            word_state.extend(one_hot)\n",
    "\n",
    "        # Encode guessed letters (binary vector)\n",
    "        guessed_state = [1.0 if c in guessed_letters else 0.0 for c in self.vocab]\n",
    "\n",
    "        # Combine states\n",
    "        state = word_state + guessed_state\n",
    "\n",
    "        return torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def guess(self, state, guessed_letters):\n",
    "        \"\"\" Get action using epsilon-greedy policy \"\"\"\n",
    "        self.total_steps += 1\n",
    "        # Epsilon decay based on total steps (slower decay)\n",
    "        self.epsilon = self.min_epsilon + \\\n",
    "            (self.initial_epsilon - self.min_epsilon) * \\\n",
    "            math.exp(-1. * self.total_steps / self.epsilon_decay_steps)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            # Exploration: Choose a random valid action\n",
    "            options = [c for c in self.vocab if c not in guessed_letters]\n",
    "            if not options: # Should not happen if game isn't over\n",
    "                options = list(self.vocab) # Fallback just in case\n",
    "            action_char = random.choice(options)\n",
    "            action_idx = self.char_to_idx[action_char]\n",
    "            return action_idx, action_char\n",
    "        else:\n",
    "            # Exploitation: Choose the best action according to the policy network\n",
    "            with torch.no_grad():\n",
    "                self.policy_net.eval() # Set to eval mode for inference\n",
    "                q_values = self.policy_net(state)\n",
    "                self.policy_net.train() # Set back to train mode\n",
    "\n",
    "                # Mask already guessed letters by setting their Q-values to negative infinity\n",
    "                for i, ch in enumerate(self.vocab):\n",
    "                    if ch in guessed_letters:\n",
    "                        q_values[i] = -float('inf')\n",
    "\n",
    "                # Handle case where all actions might be masked (shouldn't happen in valid states)\n",
    "                if torch.isinf(q_values).all():\n",
    "                     # Fallback: choose a random valid action if all are masked\n",
    "                     options = [c for c in self.vocab if c not in guessed_letters]\n",
    "                     if not options: options = list(self.vocab)\n",
    "                     action_char = random.choice(options)\n",
    "                     action_idx = self.char_to_idx[action_char]\n",
    "                     # print(\"Warning: All Q-values were -inf, choosing random valid action.\")\n",
    "                     return action_idx, action_char\n",
    "\n",
    "                best_action_idx = torch.argmax(q_values).item()\n",
    "                best_action_char = self.idx_to_char[best_action_idx]\n",
    "                return best_action_idx, best_action_char\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\" Sample batch from replay buffer and perform Double Q-learning update \"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None # Not enough samples yet\n",
    "\n",
    "        transitions = self.replay_buffer.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for details)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Create tensors from batch components\n",
    "        state_batch = torch.stack(batch.state)\n",
    "        action_batch = torch.tensor(batch.action, dtype=torch.long, device=self.device).unsqueeze(1)\n",
    "        reward_batch = torch.tensor(batch.reward, dtype=torch.float32, device=self.device)\n",
    "        next_state_batch = torch.stack(batch.next_state)\n",
    "        done_mask = torch.tensor(batch.done, dtype=torch.bool, device=self.device) # Boolean mask for final states\n",
    "\n",
    "        # --- Q-value computation ---\n",
    "        # Get Q(s_t, a) for the actions taken\n",
    "        # We get Q(s_t) from policy_net and select the column corresponding to the action taken\n",
    "        self.policy_net.train() # Ensure policy net is in train mode\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # --- Target Q-value computation (Double DQN) ---\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            # 1. Select the best action for s_{t+1} using the *policy* network\n",
    "            # We only need to compute this for non-final states\n",
    "            next_policy_q_values = self.policy_net(next_state_batch[~done_mask])\n",
    "            best_next_actions = next_policy_q_values.argmax(1).unsqueeze(1) # Get indices of max Q-values\n",
    "\n",
    "            # 2. Evaluate the Q-value of that action using the *target* network\n",
    "            next_target_q_values = self.target_net(next_state_batch[~done_mask])\n",
    "            # Use gather to select the Q-value corresponding to the best action chosen by the policy net\n",
    "            next_state_values[~done_mask] = next_target_q_values.gather(1, best_next_actions).squeeze(1)\n",
    "\n",
    "        # Compute the expected Q values: reward + gamma * Q_target(s_{t+1}, argmax_a Q_policy(s_{t+1}, a))\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # --- Loss Calculation and Optimization ---\n",
    "        # Compute Huber loss between current Q values and target Q values\n",
    "        loss = self.criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient Clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.total_optim_steps += 1\n",
    "\n",
    "        # --- Target Network Update ---\n",
    "        # Periodically update the target network by copying weights from the policy network\n",
    "        if self.total_optim_steps % self.target_update_freq == 0:\n",
    "            # print(f\"Updating target network at optimization step {self.total_optim_steps}\")\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def train_episode(self, word):\n",
    "        \"\"\" Run a single episode, store transitions, return stats \"\"\"\n",
    "        guessed_letters = set()\n",
    "        masked_word = ['_'] * len(word)\n",
    "        lives = self.max_lives\n",
    "        episode_reward = 0.0\n",
    "        steps = 0\n",
    "        step_penalty = -0.01 # Small penalty per step to encourage efficiency\n",
    "\n",
    "        current_masked_word_str = \"\".join(masked_word)\n",
    "        state = self.encode_state(current_masked_word_str, guessed_letters)\n",
    "\n",
    "        while lives > 0 and '_' in masked_word:\n",
    "            action_idx, guess_char = self.guess(state, guessed_letters)\n",
    "\n",
    "            # Determine reward and next state based on the guess\n",
    "            if guess_char in guessed_letters:\n",
    "                 # Penalty for re-guessing (wasted step)\n",
    "                 reward = -0.1\n",
    "                 next_masked_word_str = current_masked_word_str # State doesn't change\n",
    "                 # No life lost for re-guessing\n",
    "            else:\n",
    "                guessed_letters.add(guess_char)\n",
    "                correct_guess = False\n",
    "                newly_revealed_count = 0\n",
    "                next_masked_word_list = list(current_masked_word_str)\n",
    "\n",
    "                for i, char_in_word in enumerate(word):\n",
    "                    if char_in_word == guess_char:\n",
    "                        if next_masked_word_list[i] == '_':\n",
    "                           next_masked_word_list[i] = guess_char\n",
    "                           newly_revealed_count += 1\n",
    "                        correct_guess = True\n",
    "\n",
    "                next_masked_word_str = \"\".join(next_masked_word_list)\n",
    "\n",
    "                # Adjusted Reward Structure\n",
    "                if correct_guess:\n",
    "                    # Positive reward for correct guess, scaled by reveal count\n",
    "                    reward = 0.1 + 0.5 * (newly_revealed_count / len(word))\n",
    "                else:\n",
    "                    # Increased penalty for wrong guess\n",
    "                    reward = -0.3 # Was -0.2\n",
    "                    lives -= 1\n",
    "\n",
    "            # Apply step penalty\n",
    "            reward += step_penalty\n",
    "\n",
    "            # Check for game end conditions\n",
    "            game_over = (lives <= 0 or '_' not in next_masked_word_str)\n",
    "            win = (game_over and '_' not in next_masked_word_str)\n",
    "\n",
    "            # Add adjusted terminal rewards/penalties (applied on the final step)\n",
    "            if win:\n",
    "                reward += 4.0 # Was +5.0\n",
    "            elif game_over and not win:\n",
    "                reward += -3.0 # Was -2.0\n",
    "\n",
    "            episode_reward += reward\n",
    "            next_state = self.encode_state(next_masked_word_str, guessed_letters)\n",
    "\n",
    "            # Store transition in replay buffer\n",
    "            # Ensure all components are correctly formatted if needed (state should be tensor already)\n",
    "            self.replay_buffer.push(state, action_idx, next_state, reward, game_over)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            current_masked_word_str = next_masked_word_str\n",
    "            steps += 1\n",
    "\n",
    "            # Note: Learning step is now handled in the main train loop based on total_steps\n",
    "\n",
    "        # Return episode statistics\n",
    "        return episode_reward, steps, win, self.epsilon\n",
    "\n",
    "\n",
    "    def train(self, dictionary, episodes=20000, learn_every_n_steps=4, updates_per_step=1):\n",
    "        \"\"\" Main training loop \"\"\"\n",
    "        wins_in_log_interval = 0\n",
    "        total_wins = 0\n",
    "        total_episode_rewards = 0\n",
    "        total_steps_in_log_interval = 0\n",
    "        total_loss_in_log_interval = 0\n",
    "        n_losses = 0\n",
    "\n",
    "        # Pre-filter dictionary\n",
    "        valid_dictionary = [\n",
    "            word.lower() for word in dictionary\n",
    "            if len(word) <= self.max_word_len and all(c in self.vocab for c in word.lower()) and len(word) > 0\n",
    "        ]\n",
    "        print(f\"Filtered dictionary size: {len(valid_dictionary)} words\")\n",
    "        if not valid_dictionary:\n",
    "            print(\"Error: No valid words found in the dictionary for training.\")\n",
    "            return\n",
    "\n",
    "        progress_bar = tqdm(range(episodes), desc=\"Training Progress\")\n",
    "        for ep in progress_bar:\n",
    "            word = random.choice(valid_dictionary)\n",
    "\n",
    "            # Run one episode\n",
    "            ep_reward, ep_steps, win, current_epsilon = self.train_episode(word)\n",
    "\n",
    "            # Accumulate stats for logging interval\n",
    "            total_episode_rewards += ep_reward\n",
    "            total_steps_in_log_interval += ep_steps\n",
    "            if win:\n",
    "                wins_in_log_interval += 1\n",
    "                total_wins += 1\n",
    "\n",
    "            # Perform learning steps periodically based on total steps taken across episodes\n",
    "            # Check if enough steps have passed since the last learning phase\n",
    "            # Note: self.total_steps is incremented inside self.guess()\n",
    "            if self.total_steps // learn_every_n_steps > (self.total_steps - ep_steps) // learn_every_n_steps:\n",
    "                 if len(self.replay_buffer) >= self.batch_size:\n",
    "                    for _ in range(updates_per_step): # Perform multiple updates if specified\n",
    "                        loss = self.learn()\n",
    "                        if loss is not None:\n",
    "                            total_loss_in_log_interval += loss\n",
    "                            n_losses += 1\n",
    "\n",
    "\n",
    "            # Log progress periodically\n",
    "            log_interval = 100 # Log every 100 episodes\n",
    "            if (ep + 1) % log_interval == 0:\n",
    "                avg_reward = total_episode_rewards / log_interval\n",
    "                avg_loss = total_loss_in_log_interval / n_losses if n_losses > 0 else 0\n",
    "                # Calculate win rate over the *entire training* so far for a smoother trend\n",
    "                overall_win_rate = total_wins / (ep + 1)\n",
    "                # Calculate win rate *within the current log interval*\n",
    "                interval_win_rate = wins_in_log_interval / log_interval\n",
    "                avg_steps = total_steps_in_log_interval / log_interval\n",
    "\n",
    "                progress_bar.set_description(\n",
    "                    f\"Ep {ep+1}/{episodes} | Win Rate (Overall): {overall_win_rate:.2%} | \"\n",
    "                    f\"Win Rate (Last {log_interval}): {interval_win_rate:.2%} | \"\n",
    "                    f\"Avg Reward: {avg_reward:.2f} | Avg Loss: {avg_loss:.4f} | \"\n",
    "                    f\"Avg Steps: {avg_steps:.1f} | Epsilon: {current_epsilon:.4f}\"\n",
    "                )\n",
    "\n",
    "                # Reset stats for the next logging interval\n",
    "                total_episode_rewards = 0\n",
    "                total_steps_in_log_interval = 0\n",
    "                total_loss_in_log_interval = 0\n",
    "                wins_in_log_interval = 0\n",
    "                n_losses = 0\n",
    "\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\" Saves the policy network state dictionary. \"\"\"\n",
    "        torch.save(self.policy_net.state_dict(), path)\n",
    "        print(f\"Model policy network saved to {path}\")\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\" Loads the policy network state dictionary and syncs the target network. \"\"\"\n",
    "        try:\n",
    "            # Load state dict onto the correct device\n",
    "            state_dict = torch.load(path, map_location=self.device)\n",
    "            self.policy_net.load_state_dict(state_dict)\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict()) # Sync target net\n",
    "\n",
    "            # Ensure models are on the correct device (redundant if map_location worked, but safe)\n",
    "            self.policy_net.to(self.device)\n",
    "            self.target_net.to(self.device)\n",
    "\n",
    "            # Set networks to evaluation mode by default after loading\n",
    "            self.policy_net.eval()\n",
    "            self.target_net.eval()\n",
    "            print(f\"Model loaded from {path} and target network synced.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model file not found at {path}. Starting with untrained model.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model from {path}: {e}. Starting with untrained model.\")\n",
    "            # Reset networks to initial state if loading fails badly\n",
    "            self.policy_net = self.build_model().to(self.device)\n",
    "            self.target_net = self.build_model().to(self.device)\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            self.target_net.eval()\n",
    "            # Re-initialize optimizer with potentially new parameters\n",
    "            self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.optimizer.defaults['lr'], amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Filtered dictionary size: 227295 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 200000/200000 | Win Rate (Overall): 10.39% | Win Rate (Last 100): 8.00% | Avg Reward: -2.97 | Avg Loss: 0.4208 | Avg Steps: 11.4 | Epsilon: 0.0500: 100%|██████████| 200000/200000 [13:10:42<00:00,  4.22it/s]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model policy network saved to hangman_model.pth\n"
     ]
    }
   ],
   "source": [
    "with open(\"words_250000_train.txt\", \"r\") as f:\n",
    "    dictionary = f.read().splitlines()\n",
    "\n",
    "# Step 2: Initialize and train agent\n",
    "agent = HangmanRLAgent(vocab=list(\"abcdefghijklmnopqrstuvwxyz\"))\n",
    "agent.train(dictionary, episodes=200000)  # You can increase this as needed\n",
    "\n",
    "# Step 3: Save the trained model\n",
    "agent.save_model(\"hangman_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
